# Story 1.6: SSE实时搜索流

## Status
Approved

## Story
**As a** user,
**I want** to receive real-time search thinking process and final reports via SSE,
**so that** I can understand the AI analysis process and results in real-time.

## Acceptance Criteria
1. SSE库集成和连接管理实现
2. 搜索流API(/api/search/stream/{searchId})实现
3. 实时发送思考过程和最终markdown报告
4. 数据同步存储到PostgreSQL
5. JWT token验证和连接授权

**Integration Verification:**
- IV1: SSE连接不干扰现有代理中间件和CORS设置
- IV2: 并发SSE连接不影响服务器内存和性能
- IV3: 连接中断时的重连机制和状态恢复正常

## Tasks / Subtasks

- [ ] Task 1: SSE Library Integration and Setup (AC: 1)
  - [ ] Install SSE library dependency (recommend: sse-channel or express-sse adapted for Hono)
  - [ ] Create src/sse/sseManager.js for connection management
  - [ ] Implement SSE connection pool and lifecycle management
  - [ ] Add SSE configuration and environment variables
  - [ ] Test basic SSE connection establishment

- [ ] Task 2: Search Stream API Endpoint (AC: 2, 5)
  - [ ] Add GET /api/search/stream/:searchId to src/routes/search.js
  - [ ] Implement JWT authentication for SSE connections
  - [ ] Add search ownership verification for stream access
  - [ ] Implement SSE response headers and CORS compatibility
  - [ ] Handle connection upgrade and stream initialization

- [ ] Task 3: Real-time Search Result Streaming (AC: 3)
  - [ ] Create src/services/searchStreamService.js
  - [ ] Implement real-time result broadcasting to connected clients
  - [ ] Add support for THINKING result type streaming
  - [ ] Add support for REPORT result type streaming
  - [ ] Implement search status update broadcasting

- [ ] Task 4: Database Integration for Streaming (AC: 4)
  - [ ] Integrate with SearchResult model from Story 1.4
  - [ ] Implement real-time database write during streaming
  - [ ] Add search result persistence with sequence ordering
  - [ ] Implement atomic operations for stream + database updates
  - [ ] Add database transaction support for stream operations

- [ ] Task 5: Connection Management and Scalability (IV: 2)
  - [ ] Implement connection pool with configurable limits
  - [ ] Add connection timeout and cleanup mechanisms
  - [ ] Implement memory usage monitoring for SSE connections
  - [ ] Add connection rate limiting and throttling
  - [ ] Optimize SSE connection resource usage

- [ ] Task 6: CORS and Middleware Compatibility (IV: 1)
  - [ ] Ensure SSE routes work with existing CORS middleware
  - [ ] Bypass proxy middleware for SSE endpoints
  - [ ] Maintain compatibility with security headers middleware
  - [ ] Test SSE with existing authentication middleware
  - [ ] Verify health check endpoint remains unaffected

- [ ] Task 7: Connection Recovery and Resilience (IV: 3)
  - [ ] Implement client reconnection support
  - [ ] Add search state recovery for reconnected clients
  - [ ] Implement SSE heartbeat/keepalive mechanism
  - [ ] Add graceful connection termination handling
  - [ ] Implement error recovery and fallback mechanisms

- [ ] Task 8: Search Processing Integration
  - [ ] Create integration points for external search processing services
  - [ ] Implement search status update workflow (INITIATED → PROCESSING → COMPLETED)
  - [ ] Add hooks for AI service integration (future enhancement)
  - [ ] Implement search completion notification via SSE
  - [ ] Add error handling for search processing failures

- [ ] Task 9: Performance Optimization and Monitoring
  - [ ] Implement SSE connection metrics and monitoring
  - [ ] Add performance logging for stream operations
  - [ ] Optimize memory usage for long-running connections
  - [ ] Add connection cleanup and garbage collection
  - [ ] Implement stream backpressure handling

- [ ] Task 10: Testing Implementation
  - [ ] Unit tests for SSE manager and stream service
  - [ ] Integration tests for search stream API
  - [ ] WebSocket/EventSource client tests for SSE functionality
  - [ ] Performance tests for concurrent connections
  - [ ] Error handling and connection recovery tests

## Dev Notes

### Previous Stories Insights - Complete Epic Foundation

**From Story 1.1 (Database Infrastructure):**
- Prisma client established for database operations
- User model with proper relationships and indexing
- Database transaction patterns for data consistency

**From Story 1.2 (JWT Authentication):**
- JWT authentication middleware at `src/middleware/auth.js`
- JWT token validation and user extraction established
- Authentication error handling patterns for secure endpoints

**From Story 1.3 (Team Management):**
- Team permission system with Owner/Admin/Member roles
- Team access verification patterns for collaborative features
- Database transaction management for multi-user operations

**From Story 1.4 (Search Data Models):**
- Search model with complete lifecycle: INITIATED → PROCESSING → COMPLETED/FAILED
- SearchResult model with THINKING, REPORT, INTERMEDIATE, ERROR types
- Optimized database indexes: (userId, createdAt), (searchId, sequence)
- Extensible metadata system using JSON columns

**From Story 1.5 (Search List APIs):**
- High-performance query patterns with <500ms response requirements
- Pagination and sorting utilities established
- Team-based search access patterns optimized
- Frontend-compatible response formats standardized

### Technical Context from Current Project

**Current Tech Stack:**
- Framework: Hono.js v4.9.8 [Source: package.json]
- Database: Prisma + PostgreSQL with optimized search indexes [Source: Stories 1.1-1.5]
- Authentication: JWT with jsonwebtoken library [Source: Story 1.2]
- Testing: Jest v30.1.3 [Source: package.json]

**Complete Project Structure After Stories 1.1-1.5:**
```
src/
├── middleware/          # proxy.js, errorHandler.js, auth.js
├── routes/             # auth.js, teams.js, search.js
├── services/           # userService.js, teamService.js, searchService.js, searchResultService.js
├── utils/              # jwt.js, pagination.js
├── sse/                # sseManager.js (new), searchStreamService.js (new)
├── db/                 # Prisma client
└── config/             # agents.js, config.js
```

### SSE Architecture Design

**SSE Library Selection:**
Based on PRD technical decisions, using third-party SSE library. Recommended options:
- `sse-channel` - Lightweight SSE with connection management
- `express-sse` adapted for Hono.js
- Custom SSE implementation using Node.js streams

**SSE Connection Architecture:**
```javascript
// SSE Manager Structure
class SSEManager {
  constructor() {
    this.connections = new Map(); // searchId -> Set<SSEConnection>
    this.connectionPool = new Pool({ maxConnections: 1000 });
  }

  addConnection(searchId, connection) { /* ... */ }
  removeConnection(searchId, connectionId) { /* ... */ }
  broadcastToSearch(searchId, data) { /* ... */ }
  cleanup() { /* ... */ }
}
```

### Search Stream API Design

**SSE Endpoint:**
```
GET /api/search/stream/:searchId
Authorization: Bearer <jwt-token>
Accept: text/event-stream
Cache-Control: no-cache
```

**SSE Event Types:**
```javascript
// Search Status Events
data: {"type": "status", "status": "PROCESSING", "timestamp": "2024-09-29T10:00:00Z"}

// Thinking Process Events
data: {"type": "thinking", "content": "Analyzing Tesla's revenue...", "sequence": 1}

// Intermediate Results
data: {"type": "intermediate", "content": "Q3 revenue: $23.4B", "sequence": 2}

// Final Report
data: {"type": "report", "content": "# Tesla Q3 Analysis\n...", "sequence": 10}

// Error Events
data: {"type": "error", "message": "Analysis service unavailable", "code": "SERVICE_ERROR"}

// Completion Event
data: {"type": "complete", "searchId": "search_123", "totalResults": 10}
```

### Database Integration Strategy

**Real-time Database Persistence:**
```javascript
// Simultaneous SSE broadcast + database storage
const broadcastAndStore = async (searchId, resultData) => {
  await Promise.all([
    // Store in database (Story 1.4 SearchResult model)
    searchResultService.addSearchResult({
      searchId,
      resultType: resultData.type.toUpperCase(),
      content: resultData.content,
      sequence: resultData.sequence
    }),
    // Broadcast to SSE connections
    sseManager.broadcastToSearch(searchId, resultData)
  ]);
};
```

**Search Lifecycle Integration:**
1. **INITIATED**: Search created via Story 1.4 API
2. **PROCESSING**: Status update broadcasted via SSE
3. **THINKING/INTERMEDIATE**: Real-time results streamed and stored
4. **COMPLETED**: Final report delivered and search marked complete
5. **FAILED**: Error information delivered and search marked failed

### Authentication and Authorization

**JWT Token Validation for SSE:**
```javascript
// SSE-specific auth handling
app.get('/api/search/stream/:searchId', async (c) => {
  // Extract JWT from query parameter or Authorization header
  const token = c.req.query('token') || c.req.header('authorization')?.replace('Bearer ', '');

  // Validate JWT and extract user
  const user = await jwtUtils.verifyToken(token);

  // Verify search ownership
  const search = await searchService.getSearchById(searchId);
  if (search.userId !== user.id) {
    return c.json({ error: 'Unauthorized' }, 403);
  }

  // Establish SSE connection
  return createSSEConnection(searchId, user.id);
});
```

### CORS and Middleware Compatibility

**SSE Route Configuration:**
- Bypass proxy middleware for `/api/search/stream/*` routes
- Ensure CORS headers support EventSource connections
- Maintain compatibility with existing security headers
- Add SSE-specific headers: `text/event-stream`, `no-cache`

**Middleware Stack Integration:**
```javascript
// In src/app.js - SSE routes before proxy middleware
app.route('/api/search/stream', sseRoutes);  // SSE routes first
app.use('*', createProxyMiddleware());       // Proxy for other routes
```

### Performance and Scalability

**Connection Management:**
- Maximum 1000 concurrent SSE connections
- Connection timeout: 30 minutes of inactivity
- Heartbeat every 30 seconds to maintain connection
- Automatic cleanup of closed connections

**Memory Optimization:**
- Stream large content in chunks
- Implement backpressure for slow clients
- Garbage collection for closed connections
- Connection pooling with resource limits

**Monitoring and Metrics:**
- Track active SSE connection count
- Monitor memory usage per connection
- Track message delivery latency
- Alert on connection pool exhaustion

### Error Handling and Resilience

**Connection Error Scenarios:**
- Client disconnection (cleanup connection state)
- Network timeouts (implement reconnection protocol)
- Server restart (graceful connection termination)
- Database failures (fallback to in-memory buffering)

**Search Processing Errors:**
- AI service unavailability (error event to client)
- Database write failures (retry mechanism)
- Invalid search data (validation error events)
- Rate limiting exceeded (throttling messages)

### Client Integration Design

**EventSource Client Pattern:**
```javascript
// Frontend EventSource usage
const eventSource = new EventSource(`/api/search/stream/${searchId}?token=${jwtToken}`);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  switch(data.type) {
    case 'thinking': updateThinkingProcess(data); break;
    case 'report': displayFinalReport(data); break;
    case 'complete': handleSearchComplete(data); break;
  }
};

eventSource.onerror = () => {
  // Implement exponential backoff reconnection
  setTimeout(() => reconnect(), backoffDelay);
};
```

### Future Enhancement Hooks

**AI Service Integration Points:**
- Webhook endpoints for external AI processing services
- Message queue integration for asynchronous processing
- Stream processing pipeline for real-time analysis
- Multi-tenant support for different AI providers

**Advanced Streaming Features:**
- Search collaboration (multiple users watching same search)
- Search forking (branching analysis paths)
- Search templates and saved queries
- Search analytics and performance tracking

### Integration with External Services

**Search Processing Workflow:**
1. User creates search via Story 1.4 API
2. Search status updated to PROCESSING
3. External AI service begins analysis
4. AI service sends results via webhook or queue
5. Results streamed to SSE clients and stored in database
6. Search completed and final status broadcasted

**External Service Integration Points:**
- Webhook endpoints for receiving AI analysis results
- Queue-based processing for high-volume search requests
- API integration with multiple AI analysis providers
- Result validation and sanitization before streaming

### Testing Strategy

**Test Framework**: Jest [Source: package.json]
**SSE Testing Approach:**
- **Unit Tests**: SSE manager, connection lifecycle, error handling
- **Integration Tests**: End-to-end SSE flow with database persistence
- **Performance Tests**: Concurrent connections, memory usage, latency
- **Client Tests**: EventSource connection, reconnection, error handling
- **Load Tests**: 1000+ concurrent connections, stress testing

**SSE-Specific Test Tools:**
- EventSource client simulation
- WebSocket testing libraries
- Connection load testing tools
- Memory profiling for connection management

**Test File Locations:**
- `tests/sse/sseManager.test.js` (new)
- `tests/services/searchStreamService.test.js` (new)
- `tests/routes/searchStream.test.js` (new)
- `tests/integration/sseEndToEnd.test.js` (new)

### Technical Constraints

**Performance Requirements:**
- Support 1000+ concurrent SSE connections (IV2)
- Message delivery latency under 100ms
- Connection establishment under 1 second
- Memory usage under 100MB for 1000 connections

**Security Requirements:**
- JWT token validation for all SSE connections
- Search ownership verification before stream access
- Rate limiting to prevent SSE connection abuse
- Secure handling of sensitive search content

**Compatibility Requirements:**
- Must work with existing CORS middleware (IV1)
- Compatible with existing proxy routing system
- Maintains existing security header policies
- No interference with health check endpoints

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-29 | 1.0 | Initial story creation - Epic finale! | Bob (Scrum Master) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled during implementation*

### Debug Log References
*To be filled during implementation*

### Completion Notes List
*To be filled during implementation*

### File List
*To be filled during implementation*

## QA Results
*Results from QA Agent review will be added here after implementation*